{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3948626-1442-49b7-8852-63ddeefdd752",
   "metadata": {},
   "source": [
    "<h1>Excel Worker with LangGraph</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdc9ac-a0f6-4371-9c3b-ae0929701a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langgraph pandas numpy python-dotenv duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e0cf7-6c4f-48bb-ac8a-6988c17f22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3229a48-7ffa-41ef-bf32-b40723698008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Optional, Dict, List, Union\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "from langchain.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3435e7f8-152b-406c-8f66-bb109e3e6d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb871d-5303-4b92-874c-eac4aa27aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_GENERATION_PROMPT = \"\"\"You are an Excel analysis expert that generates SQL or Pandas queries to analyze data.\n",
    "\n",
    "Review the preview_data to understand the available columns and data types.\n",
    "**Evaluate the user_query. If it is simple and can be executed efficiently using Pandas, prioritize simple_dataframe_query.**\n",
    "**Use complex_duckdb_query for more complex queries that require SQL operations.**\n",
    "****Explain in the output `reasoning` value step-by-step how you designed the query itself and NOT how you decided for Pandas or DuckBD query..**\n",
    "\n",
    "Tools Available:\n",
    "1. simple_dataframe_query: For ANY operation that can be easily done with Pandas\n",
    "   Input: {\"file_name\": \"example.xlsx\", \"query\": \"df.query('column > 0').count()\"}\n",
    "   \n",
    "2. complex_duckdb_query: For complex SQL operations (GROUP BY, aggregations). ONLY for operations that cannot be done easily in Pandas\n",
    "   Input: {\"file_name\": \"example.xlsx\", \"query\": \"SELECT * FROM data\"}\n",
    "\n",
    "Data Analysis Rules:\n",
    "1. NULL/Empty Value Handling:\n",
    "   - When calculating averages across columns:\n",
    "     * Sum only non-null values\n",
    "     * Divide by count of non-null values\n",
    "   - Use CAST(column IS NOT NULL AS INTEGER) to count valid values\n",
    "   - Always use NULLIF for safe division\n",
    "   - Handle NULLs before aggregating\n",
    "\n",
    "2. Query Structure Requirements:\n",
    "   - Break complex calculations into CTEs\n",
    "   - Calculate row-level metrics first\n",
    "   - Then group and aggregate\n",
    "   - Include validation counts\n",
    "   - Example:\n",
    "     WITH base_metrics AS (\n",
    "       SELECT *,\n",
    "         COALESCE(value, 0) as clean_value,\n",
    "         CAST(value IS NOT NULL AS INTEGER) as valid_count\n",
    "     ),\n",
    "     row_metrics AS (\n",
    "       SELECT *,\n",
    "         SUM(clean_value) / NULLIF(SUM(valid_count), 0) as row_avg\n",
    "     FROM base_metrics\n",
    "     GROUP BY row_id\n",
    "     )\n",
    "\n",
    "3. DuckDB SQL Rules:\n",
    "   - Quote columns with spaces: \"Column Name\"\n",
    "   - Include all non-aggregated columns in GROUP BY\n",
    "   - Reference table as 'data'\n",
    "   - Use proper type casting\n",
    "   - Add range checks for numeric operations\n",
    "\n",
    "4. Pandas Rules:\n",
    "   - Reference DataFrame as 'df'\n",
    "   - NEVER use NumPy operations or `np.any()`, `np.all()`. Use only pure Pandas functions.\n",
    "   - Handle NULLs with fillna()\n",
    "   - Use proper data types\n",
    "   - Always store newly created columns explicitly before using them in calculations.\n",
    "   - Ensure the query executes in one go without needing multiple steps.\n",
    "   - Avoid referencing a new column inside the same assign() call if it hasn’t been created yet.\n",
    "   - The output should always be a valid Pandas statement that can execute without modification.\n",
    "\n",
    "Pandas Query Patterns - USE THESE:\n",
    "✓ Filtering: df.query('column == value')\n",
    "✓ Counting: df.query('condition').count()\n",
    "✓ Aggregation: df.query('condition').agg({'col': 'mean'})\n",
    "✓ Multiple conditions: df.query('col1 > 0 and col2 < 10')\n",
    "\n",
    "NEVER USE THESE PATTERNS:\n",
    "✗ df[df['column'] == value]  # No bracket filtering\n",
    "✗ np.any(), np.all()         # No numpy operations\n",
    "✗ df.loc[], df.iloc[]        # Avoid direct indexing\n",
    "\n",
    "**EXTREMELY IMPORTANT OUTPUT FORMAT RULES **:\n",
    "You must return ONLY a raw JSON object. \n",
    "DO NOT wrap the JSON in ```json or ``` or any other markers.\n",
    "DO NOT add any text before or after the JSON. \n",
    "DO NOT include any explanations or additional text outside the JSON.\n",
    "\n",
    "REQUIRED OUTPUT FORMAT:\n",
    "{\n",
    "    \"query\": \"<your query>\",\n",
    "    \"llm_prompt\": \"<A clear description of what the query does, reflecting the original user_input.>\",\n",
    "    \"tool\": \"<tool_name>\",\n",
    "    \"reasoning\": \"<Step-by-step explanation of how you designed the query the query>\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df468a7a-3b89-456f-81be-63633feceb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATE_QUERY_PROMPT = \"\"\"You are an AI assistant validating and improving queries for Excel data analysis.\n",
    "\n",
    "** VERY IMPORTANT RULES**:\n",
    "1. WHEN execution_result contains **error** related to Pandas query execution, then you MUST generate SQL query, given the user_query!**\n",
    "2. DO NOT, I REPEAT, DO NOT validate or change Pandas queries! Just forward them in the \"query\" value in the output JSON.\n",
    "\n",
    "Tools Available:\n",
    "1. simple_dataframe_query: (PREFERRED) For ANY operation that can be easily done with Pandas\n",
    "   Input: {\"file_name\": \"example.xlsx\", \"query\": \"df.query('column > 0').count()\"}\n",
    "   \n",
    "2. complex_duckdb_query: For complex SQL operations (GROUP BY, aggregations). ONLY for operations that cannot be done easily in Pandas\n",
    "   Input: {\"file_name\": \"example.xlsx\", \"query\": \"SELECT * FROM data\"}\n",
    "\n",
    "**SQL Query Validation:**\n",
    "1. NULL Handling:\n",
    "   - Verify NULLs are properly excluded from calculations\n",
    "   - Check for NULLIF in divisions\n",
    "   - Confirm COALESCE usage for NULL defaults\n",
    "   - Validate empty value handling in aggregations\n",
    "\n",
    "2. Aggregation Validation:\n",
    "   - All non-aggregated columns must be in GROUP BY\n",
    "   - Verify aggregation functions match business logic\n",
    "   - Check percentage calculations sum correctly\n",
    "   - Validate count metrics against row counts\n",
    "\n",
    "3. Query Structure:\n",
    "   - Use CTEs for complex calculations\n",
    "   - Break operations into logical steps:\n",
    "     * Clean and validate data\n",
    "     * Calculate row-level metrics\n",
    "     * Perform grouping and aggregation\n",
    "   - Include data validation steps\n",
    "   - Add range checks for numeric operations\n",
    "\n",
    "4. Error Prevention:\n",
    "   - Add NULLIF for division operations\n",
    "   - Include proper type casting\n",
    "   - Validate numeric operations\n",
    "   - Handle edge cases explicitly\n",
    "   \n",
    "5. Complex Calculation Patterns:\n",
    "   WITH data_validation AS (\n",
    "     -- Clean and validate input data\n",
    "   ),\n",
    "   row_metrics AS (\n",
    "     -- Calculate row-level statistics\n",
    "   ),\n",
    "   aggregated_results AS (\n",
    "     -- Perform final aggregations\n",
    "   )\n",
    "\n",
    "\n",
    "**EXTREMELY IMPORTANT OUTPUT FORMAT RULES **:\n",
    "You must return ONLY a raw JSON object. \n",
    "DO NOT wrap the JSON in ```json or ``` or any other markers.\n",
    "DO NOT add any text before or after the JSON. \n",
    "DO NOT include any explanations or additional text outside the JSON.\n",
    "\n",
    "REQUIRED OUTPUT FORMAT:\n",
    "{\n",
    "    \"tool\": \"tool_name\",\n",
    "    \"query\": \"<provide here either the original generated query or an improved version>\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df6d7e8-ba22-4699-84a2-756b020d5a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AgentState(TypedDict):\n",
    "    user_input: str\n",
    "    query: Optional[str]\n",
    "    file_name: str\n",
    "    preview_data: Optional[Dict[str, List]]\n",
    "    query_result: Optional[Dict]\n",
    "    llm_prompt: Optional[str]\n",
    "    tool: Optional[str]  # Track selected tool\n",
    "    iterations_count: int\n",
    "    error: Optional[str]  # Track errors\n",
    "\n",
    "@tool\n",
    "def load_preview_data(file_name: str) -> dict:\n",
    "    \"\"\"Examine Excel file structure and data types.\"\"\"\n",
    "    try:\n",
    "        if not file_name:\n",
    "            raise ValueError(\"File name must be provided\")\n",
    "            \n",
    "        df = pd.read_excel(os.path.join(os.getcwd(), file_name), nrows=1)\n",
    "        df = df.replace(r'^\\s*$', None, regex=True)  # Convert empty strings to None\n",
    "        df = df.replace(['nan', 'NaN', 'null'], None)  # Convert NaN strings to None\n",
    "        df = df.where(pd.notnull(df), None)  # Convert pandas NaN to None\n",
    "        \n",
    "        return {\n",
    "            \"columns\": df.columns.tolist(),\n",
    "            \"dtypes\": df.dtypes.astype(str).to_dict(),\n",
    "            \"sample_rows\": df.to_dict(orient=\"records\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to examine Excel structure: {str(e)}\")\n",
    "\n",
    "@tool\n",
    "def complex_duckdb_query(input: dict) -> dict:\n",
    "    \"\"\"Execute complex SQL operations with DuckDB.\"\"\"\n",
    "    try:\n",
    "        if not isinstance(input, dict):\n",
    "            return {\"error\": \"Input must be a dictionary\"}\n",
    "            \n",
    "        file_name = input.get(\"file_name\")\n",
    "        query = input.get(\"query\", \"\").strip()\n",
    "        \n",
    "        if not file_name or not query:\n",
    "            return {\"error\": \"Both file_name and query required\"}\n",
    "            \n",
    "        # Read and preprocess DataFrame\n",
    "        df = pd.read_excel(os.path.join(os.getcwd(), file_name))\n",
    "        df = df.replace(r'^\\s*$', None, regex=True)\n",
    "        df = df.replace(['nan', 'NaN', 'null'], None)\n",
    "        df = df.where(pd.notnull(df), None)\n",
    "        \n",
    "        # Connect to DuckDB and register DataFrame\n",
    "        con = duckdb.connect()\n",
    "        # Register as 'data' to match the prompt\n",
    "        con.register(\"data\", df)\n",
    "        \n",
    "        try:\n",
    "            # Execute query\n",
    "            result = con.execute(query).fetchdf()\n",
    "            \n",
    "            # Process results\n",
    "            if result is None:\n",
    "                return {\"result\": None}\n",
    "                \n",
    "            # Handle different result types\n",
    "            if isinstance(result, pd.DataFrame):\n",
    "                # Clean the results\n",
    "                result = result.replace([float('inf'), -float('inf')], None)\n",
    "                result = result.where(pd.notna(result), None)\n",
    "                \n",
    "                # Convert object columns to strings where needed\n",
    "                for col in result.select_dtypes(include=['object']).columns:\n",
    "                    result[col] = result[col].apply(\n",
    "                        lambda x: str(x) if x is not None else None\n",
    "                    )\n",
    "                \n",
    "                return {\n",
    "                    \"result\": {\n",
    "                        \"columns\": result.columns.tolist(),\n",
    "                        \"rows\": result.to_dict(orient=\"records\")\n",
    "                    }\n",
    "                }\n",
    "            else:\n",
    "                return {\"result\": str(result)}\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"DuckDB query error: {str(e)}\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error executing query: {str(e)}\"}\n",
    "    finally:\n",
    "        if 'con' in locals():\n",
    "            con.close()\n",
    "\n",
    "@tool\n",
    "def simple_dataframe_query(input: dict) -> dict:\n",
    "    \"\"\"Execute simple Pandas operations.\"\"\"\n",
    "    try:\n",
    "        if not isinstance(input, dict):\n",
    "            return {\"error\": \"Input must be a dictionary\"}\n",
    "            \n",
    "        file_name = input.get(\"file_name\")\n",
    "        query = input.get(\"query\", \"\").strip()\n",
    "        \n",
    "        if not file_name or not query:\n",
    "            return {\"error\": \"Both file_name and query required\"}\n",
    "            \n",
    "        # Read and preprocess DataFrame\n",
    "        df = pd.read_excel(os.path.join(os.getcwd(), file_name))\n",
    "        df = df.replace(r'^\\s*$', None, regex=True)\n",
    "        df = df.replace(['nan', 'NaN', 'null'], None)\n",
    "        df = df.where(pd.notnull(df), None)\n",
    "        \n",
    "        # Create safe execution environment\n",
    "        safe_globals = {\n",
    "            \"df\": df,\n",
    "            \"pd\": pd,\n",
    "            \"np\": np,  # Add numpy for calculations\n",
    "            \"__builtins__\": {}  # Restrict built-ins for safety\n",
    "        }\n",
    "        \n",
    "        # Execute query\n",
    "        result = eval(query, safe_globals, {})\n",
    "        \n",
    "        # Process different result types\n",
    "        if isinstance(result, pd.DataFrame):\n",
    "            result = result.replace([float('inf'), -float('inf')], None)\n",
    "            result = result.where(pd.notna(result), None)\n",
    "            \n",
    "            return {\n",
    "                \"result\": {\n",
    "                    \"type\": \"DataFrame\",\n",
    "                    \"columns\": result.columns.tolist(),\n",
    "                    \"rows\": result.to_dict(orient=\"records\")\n",
    "                }\n",
    "            }\n",
    "        elif isinstance(result, pd.Series):\n",
    "            result = result.replace([float('inf'), -float('inf')], None)\n",
    "            result = result.where(pd.notna(result), None)\n",
    "            \n",
    "            return {\n",
    "                \"result\": {\n",
    "                    \"type\": \"Series\",\n",
    "                    \"name\": result.name,\n",
    "                    \"values\": result.tolist()\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            # Handle scalar results\n",
    "            if pd.isna(result):\n",
    "                result = None\n",
    "            return {\n",
    "                \"result\": {\n",
    "                    \"type\": \"scalar\",\n",
    "                    \"value\": result\n",
    "                }\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error executing query: {str(e)}\"}\n",
    "\n",
    "\n",
    "def generate_query_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Generates initial query based on user input.\"\"\"\n",
    "    try:\n",
    "        # Ensure required fields exist\n",
    "        if not state.get(\"file_name\") or not state.get(\"user_input\"):\n",
    "            raise ValueError(\"Missing required fields: file_name or user_input\")\n",
    "            \n",
    "        # Load preview data if not present\n",
    "        if state.get(\"preview_data\") is None:\n",
    "            state[\"preview_data\"] = load_preview_data(state[\"file_name\"])\n",
    "            \n",
    "        # Generate query using LLM\n",
    "        messages = [\n",
    "            SystemMessage(content=QUERY_GENERATION_PROMPT),\n",
    "            HumanMessage(content=json.dumps({\n",
    "                \"preview_data\": state[\"preview_data\"],\n",
    "                \"user_query\": state[\"user_input\"][0] if isinstance(state[\"user_input\"], tuple) else state[\"user_input\"]\n",
    "            }, indent=2))\n",
    "        ]\n",
    "        \n",
    "        response = model_with_tools.invoke(messages)\n",
    "        print(f\"===================== Generate Query Node - Raw LLM response\\n{response.content}\\n=====================\")\n",
    "        \n",
    "        if not response or not response.content:\n",
    "            raise ValueError(\"Empty response from LLM\")\n",
    "            \n",
    "        try:\n",
    "            query_data = json.loads(response.content)\n",
    "            state[\"query\"] = query_data.get(\"query\", \"\").strip()\n",
    "            state[\"tool\"] = query_data.get(\"tool\")  # Store selected tool\n",
    "            state[\"llm_prompt\"] = query_data.get(\"llm_prompt\", \n",
    "                state[\"user_input\"][0] if isinstance(state[\"user_input\"], tuple) else state[\"user_input\"]\n",
    "            ).strip()\n",
    "            \n",
    "            # Validate required fields\n",
    "            if not state[\"query\"] or not state[\"tool\"]:\n",
    "                raise ValueError(\"Missing required fields in LLM response\")\n",
    "                \n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\"Invalid JSON response from LLM\")\n",
    "            \n",
    "        return state\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in generate_query_node: {str(e)}\")\n",
    "        state[\"error\"] = str(e)\n",
    "        state[\"query\"] = \"\"\n",
    "        state[\"tool\"] = None\n",
    "        state[\"llm_prompt\"] = state[\"user_input\"][0] if isinstance(state[\"user_input\"], tuple) else state[\"user_input\"]\n",
    "        return state\n",
    "\n",
    "def validate_and_execute_query_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Validates and executes query with error handling.\"\"\"\n",
    "    try:\n",
    "        # Check for errors from previous step\n",
    "        if state.get(\"error\"):\n",
    "            logger.error(f\"Previous error detected: {state['error']}\")\n",
    "            return state\n",
    "            \n",
    "        # Ensure required fields exist\n",
    "        if not state.get(\"query\") or not state.get(\"tool\"):\n",
    "            state[\"error\"] = \"Missing query or tool selection\"\n",
    "            return state\n",
    "            \n",
    "        # Ensure preview data exists\n",
    "        if state.get(\"preview_data\") is None:\n",
    "            state[\"preview_data\"] = load_preview_data(state[\"file_name\"])\n",
    "            \n",
    "        # Prepare validation message\n",
    "        messages = [\n",
    "            SystemMessage(content=VALIDATE_QUERY_PROMPT),\n",
    "            HumanMessage(content=json.dumps({\n",
    "                \"preview_data\": state[\"preview_data\"],\n",
    "                \"llm_query\": state[\"query\"],\n",
    "                \"user_query\": state[\"llm_prompt\"],\n",
    "                \"execution_result\": state.get(\"query_result\")\n",
    "            }))\n",
    "        ]\n",
    "        print(f\"===================== Validate and Execute Query Node - Messages\\n{messages}\\n=====================\")\n",
    "        \n",
    "        # Get and parse LLM response\n",
    "        response = model_with_tools.invoke(messages)\n",
    "        print(f\"===================== Validate and Execute Query Node - Raw LLM response\\n{response.content}\\n=====================\")\n",
    "        \n",
    "        try:\n",
    "            response_dict = json.loads(response.content)\n",
    "            \n",
    "            # Update state with validated query and tool\n",
    "            state[\"query\"] = response_dict.get(\"query\", \"\").strip()\n",
    "            state[\"tool\"] = response_dict.get(\"tool\")\n",
    "            \n",
    "            if not state[\"query\"] or not state[\"tool\"]:\n",
    "                raise ValueError(\"Invalid validation response\")\n",
    "                \n",
    "        except (json.JSONDecodeError, ValueError) as e:\n",
    "            state[\"error\"] = f\"Validation error: {str(e)}\"\n",
    "            return state\n",
    "            \n",
    "        # Execute query with appropriate tool\n",
    "        # Fixed: Properly structure the input dictionary\n",
    "        input_data = {\n",
    "            \"input\": {  # Add this nesting level\n",
    "                \"file_name\": state[\"file_name\"],\n",
    "                \"query\": state[\"query\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if state[\"tool\"] == \"simple_dataframe_query\":\n",
    "            result = simple_dataframe_query.invoke(input_data)  # Use invoke instead of direct call\n",
    "        elif state[\"tool\"] == \"complex_duckdb_query\":\n",
    "            result = complex_duckdb_query.invoke(input_data)  # Use invoke instead of direct call\n",
    "        else:\n",
    "            state[\"error\"] = f\"Unknown tool: {state['tool']}\"\n",
    "            return state\n",
    "            \n",
    "        # Update state\n",
    "        state[\"query_result\"] = result\n",
    "        state[\"iterations_count\"] = state.get(\"iterations_count\", 0) + 1\n",
    "        \n",
    "        return state\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in validate_and_execute_query_node: {str(e)}\")\n",
    "        state[\"error\"] = str(e)\n",
    "        return state\n",
    "\n",
    "\n",
    "def next_step(state: AgentState) -> str:\n",
    "    \"\"\"Determines next step in workflow with improved error handling.\"\"\"\n",
    "    iterations_count = state.get(\"iterations_count\", 0)\n",
    "    query_result = state.get(\"query_result\", {})\n",
    "    \n",
    "    # Check for errors in query_result\n",
    "    has_error = (\n",
    "        isinstance(query_result, dict) and \n",
    "        \"error\" in query_result and \n",
    "        query_result[\"error\"] is not None\n",
    "    )\n",
    "    \n",
    "    if has_error and iterations_count <= 3:\n",
    "        logger.info(\"Retrying validation (iteration %d)\", iterations_count)\n",
    "        return \"validate_and_execute_query_node\"\n",
    "    \n",
    "    logger.info(\"Workflow complete\")\n",
    "    return END\n",
    "\n",
    "# Function to create and initialize the graph\n",
    "def create_agent_graph(memory: Optional[MemorySaver] = None) -> StateGraph:\n",
    "    builder = StateGraph(AgentState)\n",
    "    \n",
    "    # Add nodes\n",
    "    builder.add_node(\"generate_query_node\", generate_query_node)\n",
    "    builder.add_node(\"validate_and_execute_query_node\", validate_and_execute_query_node)\n",
    "    \n",
    "    # Add edges\n",
    "    builder.add_edge(\"generate_query_node\", \"validate_and_execute_query_node\")\n",
    "    builder.add_conditional_edges(\n",
    "        \"validate_and_execute_query_node\",\n",
    "        next_step,\n",
    "        {END: END, \"validate_and_execute_query_node\": \"validate_and_execute_query_node\"}\n",
    "    )\n",
    "    \n",
    "    # Set entry/exit points\n",
    "    builder.set_entry_point(\"generate_query_node\")\n",
    "    builder.set_finish_point(\"validate_and_execute_query_node\")\n",
    "    \n",
    "    return builder.compile(checkpointer=memory)\n",
    "\n",
    "# Helper function to pretty print events\n",
    "def pretty_print_event(event: dict):\n",
    "    \"\"\"Pretty-print a single event (dictionary).\"\"\"\n",
    "    print(json.dumps(event, indent=2))\n",
    "\n",
    "def run_agent_query(user_input: str, file_name: str):\n",
    "    \"\"\"Run a query through the agent.\"\"\"\n",
    "    memory = MemorySaver()\n",
    "    graph = create_agent_graph(memory)\n",
    "    \n",
    "    thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "    initial_state = {\n",
    "        \"user_input\": user_input,\n",
    "        \"file_name\": file_name,\n",
    "        \"iterations_count\": 0,\n",
    "        \"query\": None,\n",
    "        \"preview_data\": None,\n",
    "        \"query_result\": None,\n",
    "        \"llm_prompt\": None,\n",
    "        \"tool\": None,\n",
    "        \"error\": None\n",
    "    }\n",
    "    \n",
    "    events = []\n",
    "    for event in graph.stream(initial_state, thread):\n",
    "        events.append(event)\n",
    "        pretty_print_event(event)\n",
    "        \n",
    "    # Return final state\n",
    "    return events[-1] if events else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfad68a-ce16-4b95-87b7-d248b627d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "graph = create_agent_graph()\n",
    "Image(graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24be5df-8977-410e-b0f3-a6954abe1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize exactly as in original\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "model_with_tools = model.bind_tools(\n",
    "    [simple_dataframe_query, complex_duckdb_query],\n",
    "    parallel_tool_calls=False\n",
    ")\n",
    "\n",
    "# Run query\n",
    "user_query = \"Which ticker symbol have avg performance above 50% given the performance of a ticker symbol is spread in the months 1 through 13. Ignore empty months in the calculation!\",\n",
    "file_name = \"ipo_data.xlsx\"\n",
    "run_agent_query(user_query, file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
